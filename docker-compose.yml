services:
  frontend:
    build:
      context: .
      dockerfile: ./frontend/Dockerfile
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - WDS_SOCKET_PORT=0
      - WATCHPACK_POLLING=true
    networks:
      - gofies
    healthcheck:
      test: ["CMD", "curl", "-f", "-k", "https://nginx/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  backend:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    volumes:
      - ./backend:/app
      - /app/node_modules
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_INITDB_ROOT_USERNAME}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_INITDB_ROOT_PASSWORD}
      - MONGO_INITDB_DATABASE=${MONGO_INITDB_DATABASE}
    networks:
      - gofies
    depends_on:
      - mongo
    healthcheck:
      test: ["CMD", "curl", "-f", "-k", "https://nginx/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  mongo:
    image: mongo:8.0
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
    networks:
      - gofies
    # will need to add this volume later to map the database to the local host for presistence of data
    # volumes:
    #   - ./database/mongodb/db_data:/data/db
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5

  citus-master:
    image: "citusdata/citus:12.1.3"
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
      CITUS_WORKER_COUNT: "${CITUS_WORKER_COUNT}"
    networks:
      - gofies
    volumes:
      #- ./database/postgresql/db_data:/var/lib/postgresql/data # will need to add this volume later to map the database to the local host for presistence of data
      - ./database/citus:/data/citus
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 5s
      timeout: 5s
      retries: 5
    command: ["postgres", "-c", "max_connections=100"]

  citus-worker:
    image: "citusdata/citus:12.1.3"
    deploy:
      mode: replicated
      replicas: "${CITUS_WORKER_COUNT}"
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
    networks:
      - gofies
    volumes:
      #- ./database/postgresql/db_data:/var/lib/postgresql/data # will need to add this volume later to map the database to the local host for presistence of data
      - ./database/citus:/data/citus
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 5s
      timeout: 5s
      retries: 5

  citus-init:
    image: "citusdata/citus:12.1.3"
    volumes:
      #- ./database/postgresql/db_data:/var/lib/postgresql/data # will need to add this volume later to map the database to the local host for presistence of data
      - ./database/citus:/data/citus
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
      CITUS_WORKER_COUNT: "${CITUS_WORKER_COUNT}"
    networks:
      - gofies
    command: "sh -c 'sed -i \"s/$$\r$$//\" /data/citus/init.sh && chmod +x /data/citus/init.sh && /data/citus/init.sh'"
    depends_on:
      citus-master:
        condition: service_healthy
      citus-worker:
        condition: service_healthy

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx-logs:/var/log/nginx
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    networks:
      - gofies
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 5s
      timeout: 5s
      retries: 5

networks:
  gofies:
    driver: bridge
